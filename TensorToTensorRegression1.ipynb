{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorly library\n",
    "import tensorly as tl\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TENSOR TO TENSOR REGRESSION v1    \n",
    "  Inputs:\n",
    "    X - Input Tensor, shape (N, I_1, ..., I_{M1})\n",
    "    Y - Output Tensor, shape (N, J_1, ..., J_{M2})\n",
    "    rank - The rank of the solution weights matrix\n",
    "    lambda_reg - how much l2 regularization to use\n",
    "    return_factors - whether to include the weight matrix factors along with the weight matrix itself\n",
    "    eps - the precision of our estimate\n",
    "    \n",
    "  Outputs:\n",
    "    W - weight matrix tensor, shape (I_1, ..., I_{M1}, J_1, ..., J_{M2})\n",
    "    factors - List of CP factors which forms the CP decomposition of W. Will only be returned if return_factors==True\n",
    "\"\"\"\n",
    "def tensor_to_tensor_regression_1(X, Y, rank, lambda_reg=0.0, return_factors=False, eps=1e-4):\n",
    "    # Check that the N is consistent between X and Y tensors\n",
    "    if X.shape[0] != Y.shape[0]:\n",
    "        print(\"Wrong leading dimensions for tensors X and Y\")\n",
    "    \n",
    "    # Number of examples (leading dimension in the tensor)\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Setup the sizes of the X and Y matrices\n",
    "    I = reduce(lambda x, y: x * y, X.shape[1:])\n",
    "    J = reduce(lambda x, y: x * y, Y.shape[1:])\n",
    "    \n",
    "    # Initialize the u_r and v_r vectors which we will directly optimize\n",
    "    u_rs = [np.random.random(I)-0.5 for r in range(rank)]\n",
    "    v_rs = [np.random.random(J)-0.5 for r in range(rank)]\n",
    "    \n",
    "    # Matricize X and Y\n",
    "    X_mat = tl.unfold(X, mode=0)\n",
    "    Y_mat = tl.unfold(Y, mode=0)\n",
    "    \n",
    "    # Precompute some repeatedly used matrices\n",
    "    XtXpL = np.matmul(np.transpose(X_mat), X_mat) + lambda_reg * np.identity(I)\n",
    "    XtXpL_inv = np.linalg.inv(XtXpL)\n",
    "    XtY = np.matmul(np.transpose(X_mat), Y_mat)\n",
    "    YtX = np.transpose(XtY)\n",
    "    \n",
    "    steps = 0\n",
    "    prev_error = -1\n",
    "    errors = [] # TODO: Remove for production (only used for testing)\n",
    "    # Keep optimizing until the error has converged\n",
    "    while True:\n",
    "        # For every rank of the weights matrix\n",
    "        for r in range(rank):\n",
    "            \n",
    "            # Update all the u_rs first\n",
    "            u_comp = np.zeros(I)\n",
    "            # Do some ugly linear algebra\n",
    "            for r1 in range(rank):\n",
    "                # Only want cases where r1 != r\n",
    "                if r1 == r:\n",
    "                    continue\n",
    "                # Computing the summation in equations (13)\n",
    "                u_comp += np.dot(v_rs[r1], v_rs[r]) * np.matmul(XtXpL, u_rs[r1])\n",
    "            # More ugly linear algebra, finishing off the calculation from (13) \n",
    "            u_rs[r] = np.matmul(XtXpL_inv, np.matmul(XtY, v_rs[r])-(u_comp / 2)) / np.dot(v_rs[r], v_rs[r])\n",
    "            \n",
    "            # Now update all the v_rs\n",
    "            v_comp = np.zeros(J)\n",
    "            # Do some ugly linear algebra\n",
    "            for r1 in range(rank):\n",
    "                # Only want cases where r1 != r\n",
    "                if r1 == r:\n",
    "                    continue\n",
    "                v_comp += np.matmul(u_rs[r], np.matmul(XtXpL, u_rs[r1])) * v_rs[r1]\n",
    "            v_rs[r] = (np.matmul(YtX, u_rs[r]) - (v_comp / 2)) / np.matmul(u_rs[r], np.matmul(XtXpL, u_rs[r]))\n",
    "        \n",
    "        # Compute the new error, this time ignoring regularization\n",
    "        W_mat = np.zeros((I, J))\n",
    "        for r in range(rank):\n",
    "            # Add each W_r\n",
    "            W_mat += tl.kron(u_rs[r], v_rs[r]).reshape(I, J)\n",
    "        # Use MSE to normalize the by the size of the tensor\n",
    "        error = np.square(Y_mat - np.matmul(X_mat, W_mat)).mean() \n",
    "        errors.append(error)\n",
    "        # Determine if we have converged\n",
    "        if prev_error > 0 and abs(prev_error - error) < eps:\n",
    "            print(\"Converged after\", steps, \"steps. Final Error:\", error)\n",
    "            break\n",
    "        else:\n",
    "            print(\"Step:\", steps, \"Error:\", error)\n",
    "        # Reset the previous error\n",
    "        prev_error = error\n",
    "        # Next step\n",
    "        steps += 1\n",
    "        \n",
    "    \n",
    "    # TODO: Add functionality to make the factors\n",
    "    \n",
    "    return tl.reshape(W_mat, (X.shape[1:]) + (Y.shape[1:])) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 0.6513703669924733\n",
      "Step: 1 Error: 0.6956659064715023\n",
      "Step: 2 Error: 0.6369962188748379\n",
      "Step: 3 Error: 0.6026667121212064\n",
      "Step: 4 Error: 0.5945693295385142\n",
      "Converged after 5 steps. Final Error: 0.5945898932413508\n",
      "(5, 7, 2, 3)\n",
      "0.5945898932413508\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "X = np.random.random((N, 5, 7))\n",
    "Y = np.zeros((N, 2, 3))\n",
    "# Setup Y tensor with some dummy data\n",
    "for n in range(N):\n",
    "    x = X[n]\n",
    "    Y[n] = np.array([[x[0, 0] + x[1, 1] , 2 * x[1, 0] - x[3, 2], (x[4, 5] + 1) ** 2],\n",
    "                    [-x[1, 6] + 3 * x[1, 5] ,  - x[0, 6] - x[3, 3], (x[2, 5] + 2) ** 2]])\n",
    "# Now fit it\n",
    "W = tensor_to_tensor_regression_1(X, Y, 5, lambda_reg=100)\n",
    "print(W.shape)\n",
    "Y_pred = tl.tenalg.contract(X, range(1, tl.ndim(X)), W, range(tl.ndim(X) - 1))\n",
    "print(np.square(Y_pred - Y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.ndim(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
